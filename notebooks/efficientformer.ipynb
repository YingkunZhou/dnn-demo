{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets --trusted-host pypi.org --trusted-host pypi.python.org --trusted-host=files.pythonhosted.org\n",
    "%pip install wget\n",
    "%pip install torch_tensorrt\n",
    "%pip install tensorrt\n",
    "%pip install timm\n",
    "%pip install torchsummary\n",
    "%pip install pytorch-quantization --extra-index-url https://pypi.ngc.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0+nv23.05\n",
      "1.4.0\n",
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "print(torch.__version__)\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models, datasets\n",
    "import torch_tensorrt\n",
    "print(torch_tensorrt.__version__)\n",
    "\n",
    "import pytorch_quantization\n",
    "from pytorch_quantization import nn as quant_nn\n",
    "from pytorch_quantization import quant_modules\n",
    "from pytorch_quantization.tensor_quant import QuantDescriptor\n",
    "from pytorch_quantization import calib\n",
    "print(pytorch_quantization.__version__)\n",
    "\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "from timm.data import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import time\n",
    "import numpy as np\n",
    "import wget\n",
    "import tarfile\n",
    "import shutil\n",
    "from PIL import Image\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model skeleton definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/67631/how-can-i-import-a-module-dynamically-given-the-full-path\n",
    "import importlib.util\n",
    "spec = importlib.util.spec_from_file_location(\"module.name\", os.environ['HOME']+\"/work/transfer-learning/EfficientFormerV2/model.py\")\n",
    "efficientformerv2 = importlib.util.module_from_spec(spec)\n",
    "sys.modules[\"module.name\"] = efficientformerv2\n",
    "spec.loader.exec_module(efficientformerv2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"efficientformerv2_s1\"\n",
    "orig_wight = \"/home/loongson/work/orin-demo/eformer_s1_450.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_model = getattr(efficientformerv2, model_name)\n",
    "model = create_model(num_classes=10).cuda()\n",
    "weights_dict = torch.load(orig_wight)\n",
    "weights_dict = weights_dict[\"model\"] if \"model\" in weights_dict else weights_dict\n",
    "for k in list(weights_dict.keys()):\n",
    "    if \"head.weight\" in k or \"head.bias\" in k:\n",
    "    # if \"dist_head\" in k:\n",
    "        del weights_dict[k]\n",
    "model.load_state_dict(weights_dict, strict=False)\n",
    "#model = timm.create_model(model_name+'.snap_dist_in1k', pretrained=True)\n",
    "#model.head = nn.Linear(224, 10)\n",
    "#model.head_dist = nn.Linear(224, 10)\n",
    "#model = model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose(\n",
    "    [transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)])\n",
    "# [N, C, H, W]\n",
    "img = data_transform(Image.open(\"../daisy.jpg\"))\n",
    "# expand batch dimension\n",
    "img = torch.unsqueeze(img, dim=0).cuda()\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    out = model(img)\n",
    "    predict = torch.squeeze(out).cpu()\n",
    "    for i in [985]: # daisy\n",
    "        print(\"class: {:10}   prob: {:.11}\".format(str(i), predict[i].numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, (3, 224, 224))\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare dataset and begin fune-tuning & transfer learning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(DATA_DIR):\n",
    "    if os.path.exists(DATA_DIR):\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, 'imagenette2-320')):\n",
    "            url = 'https://s3.amazonaws.com/fast-ai-imageclas/imagenette2-320.tgz'\n",
    "            wget.download(url)\n",
    "            # open file\n",
    "            file = tarfile.open('imagenette2-320.tgz')\n",
    "            # extracting file\n",
    "            file.extractall(DATA_DIR)\n",
    "            file.close()\n",
    "    else:\n",
    "        print(\"This directory doesn't exist. Create the directory and run again\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"./data\"):\n",
    "    os.mkdir(\"./data\")\n",
    "download_data(\"./data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### construct train and valid data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define main data directory\n",
    "DATA_DIR = './data/imagenette2-320' \n",
    "# Define training and validation data paths\n",
    "TRAIN_DIR = os.path.join(DATA_DIR, 'train') \n",
    "VAL_DIR = os.path.join(DATA_DIR, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Performing Transformations on the dataset and defining training and validation dataloaders\n",
    "transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD)\n",
    "            ])\n",
    "train_dataset = datasets.ImageFolder(TRAIN_DIR, transform=transform)\n",
    "val_dataset = datasets.ImageFolder(VAL_DIR, transform=transform)\n",
    "calib_dataset = torch.utils.data.random_split(val_dataset, [2901, 1024])[1]\n",
    "\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "calib_dataloader = data.DataLoader(calib_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "\n",
    "# Use cross entropy loss for classification\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define functions for training, evalution, saving checkpoint and train parameter setting function\n",
    "def train(model, dataloader, crit, opt, epoch):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch, (data, labels) in enumerate(dataloader):\n",
    "        data, labels = data.cuda(), labels.cuda(non_blocking=True)\n",
    "        opt.zero_grad()\n",
    "        out = model(data)\n",
    "        loss = crit(out, labels)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        running_loss += loss.item()\n",
    "        if batch % 100 == 99:\n",
    "            print(\"Batch: [%5d | %5d] loss: %.3f\" % (batch + 1, len(dataloader), running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "        \n",
    "def evaluate(model, dataloader, crit, non_blocking=True):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    loss = 0.0\n",
    "    class_probs = []\n",
    "    class_preds = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, labels in dataloader:\n",
    "            data, labels = data.cuda(), labels.cuda(non_blocking=non_blocking)\n",
    "            out = model(data)\n",
    "            loss += crit(out, labels)\n",
    "            preds = torch.max(out, 1)[1]\n",
    "            class_probs.append([F.softmax(i, dim=0) for i in out])\n",
    "            class_preds.append(preds)\n",
    "            total += labels.size(0)\n",
    "            correct += (preds == labels).sum().item()\n",
    "\n",
    "    evaluate_probs = torch.cat([torch.stack(batch) for batch in class_probs])\n",
    "    evaluate_preds = torch.cat(class_preds)\n",
    "\n",
    "    return loss / total, correct / total\n",
    "\n",
    "def save_checkpoint(state, ckpt_path=\"checkpoint.pth\"):\n",
    "    torch.save(state, ckpt_path)\n",
    "    print(\"Checkpoint saved\")\n",
    "    \n",
    "cudnn.benchmark = True\n",
    "def cpu_timestamp(*args, **kwargs):\n",
    "    # perf_counter returns time in seconds\n",
    "    return time.perf_counter()\n",
    "\n",
    "# Helper function to benchmark the model\n",
    "def benchmark(model, dtype='fp32', nwarmup=50, nruns=1000):\n",
    "    data = iter(val_dataloader)\n",
    "    images, _ = next(data)\n",
    "    # expand batch dimension to [N, C, H, W]\n",
    "    img = torch.unsqueeze(images[0], dim=0).cuda()\n",
    "\n",
    "    if dtype=='fp16':\n",
    "        img = img.half() # FIXME?\n",
    "        \n",
    "    with torch.no_grad():\n",
    "        for _ in range(nwarmup):\n",
    "            features = model(img)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    time_min = 1e5\n",
    "    time_avg = 0\n",
    "    time_max = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(nruns):\n",
    "            start_time = cpu_timestamp()\n",
    "            output = model(img)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = cpu_timestamp()\n",
    "            time = (end_time - start_time)\n",
    "            time_min = time if time < time_min else time_min\n",
    "            time_max = time if time > time_max else time_max\n",
    "            time_avg += time\n",
    "    print(\"min = {:7.2f} ms\\tmax = {:7.2f} ms\\tavg = {:7.2f} ms\".format(1000*time_min, 1000*time_max, 1000*time_avg/nruns))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [    1 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 2.291\n",
      "Batch: [  200 |   295] loss: 2.232\n",
      "Test Loss: 0.03353 Test Acc: 32.33%\n",
      "Checkpoint saved\n",
      "Epoch: [    2 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 2.130\n",
      "Batch: [  200 |   295] loss: 2.079\n",
      "Test Loss: 0.03132 Test Acc: 48.39%\n",
      "Checkpoint saved\n",
      "Epoch: [    3 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 1.975\n",
      "Batch: [  200 |   295] loss: 1.913\n",
      "Test Loss: 0.02881 Test Acc: 60.02%\n",
      "Checkpoint saved\n",
      "Epoch: [    4 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 1.809\n",
      "Batch: [  200 |   295] loss: 1.743\n",
      "Test Loss: 0.02603 Test Acc: 66.52%\n",
      "Checkpoint saved\n",
      "Epoch: [    5 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 1.635\n",
      "Batch: [  200 |   295] loss: 1.580\n",
      "Test Loss: 0.02353 Test Acc: 72.87%\n",
      "Checkpoint saved\n",
      "Epoch: [    6 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 1.486\n",
      "Batch: [  200 |   295] loss: 1.431\n",
      "Test Loss: 0.02113 Test Acc: 76.20%\n",
      "Checkpoint saved\n",
      "Epoch: [    7 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 1.344\n",
      "Batch: [  200 |   295] loss: 1.305\n",
      "Test Loss: 0.01907 Test Acc: 79.25%\n",
      "Checkpoint saved\n",
      "Epoch: [    8 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 1.226\n",
      "Batch: [  200 |   295] loss: 1.218\n",
      "Test Loss: 0.01763 Test Acc: 81.10%\n",
      "Checkpoint saved\n",
      "Epoch: [    9 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 1.162\n",
      "Batch: [  200 |   295] loss: 1.100\n",
      "Test Loss: 0.01639 Test Acc: 82.17%\n",
      "Checkpoint saved\n",
      "Epoch: [   10 /    10] LR: 0.000100\n",
      "Batch: [  100 |   295] loss: 1.061\n",
      "Batch: [  200 |   295] loss: 1.037\n",
      "Test Loss: 0.01492 Test Acc: 83.56%\n",
      "Checkpoint saved\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 3 epochs to attain an acceptable accuracy.\n",
    "num_epochs=10\n",
    "best_acc = 0.\n",
    "# Declare Learning rate\n",
    "lr = 0.0001\n",
    "#  Use SGD optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, lr))\n",
    "\n",
    "    train(model, train_dataloader, criterion, optimizer, epoch)\n",
    "    test_loss, test_acc = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "    print(\"Test Loss: {:.5f} Test Acc: {:.2f}%\".format(test_loss, 100 * test_acc))\n",
    "    if best_acc < test_acc:\n",
    "        best_acc = test_acc\n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'model': model.state_dict(),\n",
    "            'acc': best_acc,\n",
    "            'opt_state_dict': optimizer.state_dict()\n",
    "            }, ckpt_path=model_name+\"_base_ckpt.pth\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## begin eval and benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientFormerV2(\n",
       "  (patch_embed): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): GELU(approximate='none')\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): GELU(approximate='none')\n",
       "  )\n",
       "  (network): ModuleList(\n",
       "    (0): Sequential(\n",
       "      (0): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (mid_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (mid_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)\n",
       "          (mid_norm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): EmbeddingAsubN(\n",
       "      (proj): Conv2d(32, 48, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (2): Sequential(\n",
       "      (0): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "          (mid_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "          (mid_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192)\n",
       "          (mid_norm): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(48, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): EmbeddingAsubN(\n",
       "      (proj): Conv2d(48, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (norm): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (4): Sequential(\n",
       "      (0): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480)\n",
       "          (mid_norm): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480)\n",
       "          (mid_norm): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(120, 360, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(360, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)\n",
       "          (mid_norm): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(120, 360, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(360, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)\n",
       "          (mid_norm): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(120, 360, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(360, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)\n",
       "          (mid_norm): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(120, 360, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(360, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=360)\n",
       "          (mid_norm): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (6): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480)\n",
       "          (mid_norm): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (7): AttnFFN(\n",
       "        (token_mixer): Attention4D(\n",
       "          (stride_conv): Sequential(\n",
       "            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "          (q): Sequential(\n",
       "            (0): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (k): Sequential(\n",
       "            (0): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v): Sequential(\n",
       "            (0): Conv2d(120, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v_local): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (talking_head1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (talking_head2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj): Sequential(\n",
       "            (0): GELU(approximate='none')\n",
       "            (1): Conv2d(1024, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480)\n",
       "          (mid_norm): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (8): AttnFFN(\n",
       "        (token_mixer): Attention4D(\n",
       "          (stride_conv): Sequential(\n",
       "            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120)\n",
       "            (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (upsample): Upsample(scale_factor=2.0, mode='bilinear')\n",
       "          (q): Sequential(\n",
       "            (0): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (k): Sequential(\n",
       "            (0): Conv2d(120, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v): Sequential(\n",
       "            (0): Conv2d(120, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v_local): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (talking_head1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (talking_head2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj): Sequential(\n",
       "            (0): GELU(approximate='none')\n",
       "            (1): Conv2d(1024, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(120, 480, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(480, 120, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(480, 480, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=480)\n",
       "          (mid_norm): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "    (5): EmbeddingAsub(\n",
       "      (attn): Attention4DDownsample(\n",
       "        (q): LGQuery(\n",
       "          (pool): AvgPool2d(kernel_size=1, stride=2, padding=0)\n",
       "          (local): Sequential(\n",
       "            (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=120)\n",
       "          )\n",
       "          (proj): Sequential(\n",
       "            (0): Conv2d(120, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (k): Sequential(\n",
       "          (0): Conv2d(120, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (v): Sequential(\n",
       "          (0): Conv2d(120, 512, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (v_local): Sequential(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=512)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (proj): Sequential(\n",
       "          (0): GELU(approximate='none')\n",
       "          (1): Conv2d(512, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (conv): Conv2d(120, 224, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "      (bn): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (6): Sequential(\n",
       "      (0): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)\n",
       "          (mid_norm): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (1): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)\n",
       "          (mid_norm): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (2): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(224, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(672, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672)\n",
       "          (mid_norm): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (3): FFN(\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(224, 672, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(672, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=672)\n",
       "          (mid_norm): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (4): AttnFFN(\n",
       "        (token_mixer): Attention4D(\n",
       "          (q): Sequential(\n",
       "            (0): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (k): Sequential(\n",
       "            (0): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v): Sequential(\n",
       "            (0): Conv2d(224, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v_local): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (talking_head1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (talking_head2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj): Sequential(\n",
       "            (0): GELU(approximate='none')\n",
       "            (1): Conv2d(1024, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)\n",
       "          (mid_norm): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "      (5): AttnFFN(\n",
       "        (token_mixer): Attention4D(\n",
       "          (q): Sequential(\n",
       "            (0): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (k): Sequential(\n",
       "            (0): Conv2d(224, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v): Sequential(\n",
       "            (0): Conv2d(224, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (v_local): Sequential(\n",
       "            (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (talking_head1): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (talking_head2): Conv2d(8, 8, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (proj): Sequential(\n",
       "            (0): GELU(approximate='none')\n",
       "            (1): Conv2d(1024, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Conv2d(224, 896, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (act): GELU(approximate='none')\n",
       "          (fc2): Conv2d(896, 224, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (mid): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)\n",
       "          (mid_norm): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm1): BatchNorm2d(896, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (norm2): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): BatchNorm2d(224, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (head): Linear(in_features=224, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_dict = torch.load(model_name+\"_base_ckpt.pth\") # map_location=args.device\n",
    "weights_dict = weights_dict[\"model\"] if \"model\" in weights_dict else weights_dict\n",
    "create_model = getattr(efficientformerv2, model_name)\n",
    "model = create_model(num_classes=10).cuda()\n",
    "model.load_state_dict(weights_dict)\n",
    "model.eval()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### raw model precision and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_s1 Baseline accuracy: 83.56%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the baseline model\n",
    "test_loss, test_acc = evaluate(model, val_dataloader, criterion)\n",
    "print(model_name+\" Baseline accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =   26.74 ms\tmax =   43.05 ms\tavg =   27.03 ms\n"
     ]
    }
   ],
   "source": [
    "#benchmark the performance of the baseline model\n",
    "benchmark(model, nruns=400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### trace model precision and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    traced_model = torch.jit.trace(model, torch.randn((1,3,224,224)).cuda())\n",
    "    torch.jit.save(traced_model, model_name+\"_base.jit.pt\")\n",
    "traced_model = torch.jit.load(model_name+\"_base.jit.pt\").eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_s1 trace accuracy: 83.56%\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the baseline model\n",
    "traced_model = torch.jit.trace(model, torch.randn((1,3,224,224)).cuda()).eval()\n",
    "test_loss, test_acc = evaluate(traced_model, val_dataloader, criterion)\n",
    "print(model_name+\" trace accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =   14.45 ms\tmax =   16.71 ms\tavg =   14.70 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(traced_model, nruns=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### script model precision and performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_s1 script accuracy: 83.56%\n"
     ]
    }
   ],
   "source": [
    "script_model = torch.jit.script(model).eval()\n",
    "test_loss, test_acc = evaluate(script_model, val_dataloader, criterion)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =   15.39 ms\tmax =   16.88 ms\tavg =   15.64 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(script_model, nruns=100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile to Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine val dataloader batch size\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRT FP32 model on CUDA GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_s1 script accuracy: 83.49%\n"
     ]
    }
   ],
   "source": [
    "# benchmark the performance of the baseline TRT model (TRT FP32 Model)\n",
    "compile_spec = {\n",
    "    \"inputs\": [torch_tensorrt.Input([1, 3, 224, 224])],\n",
    "    }\n",
    "\n",
    "script_fp32_model = torch_tensorrt.compile(script_model, **compile_spec)\n",
    "test_loss, test_acc = evaluate(script_fp32_model, val_dataloader, criterion)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =    2.39 ms\tmax =    6.42 ms\tavg =    2.63 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(script_fp32_model, nruns=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Float64 to Float32\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Float64 to Float32\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Float64 to Float32\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Float64 to Float32\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Float64 to Float32\n",
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_s1 script accuracy: 83.49%\n"
     ]
    }
   ],
   "source": [
    "trace_fp32_model = torch_tensorrt.compile(traced_model, **compile_spec)\n",
    "test_loss, test_acc = evaluate(trace_fp32_model, val_dataloader, criterion)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =    2.33 ms\tmax =    5.90 ms\tavg =    2.56 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(trace_fp32_model, nruns=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_s1 script accuracy: 83.49%\n"
     ]
    }
   ],
   "source": [
    "trt_fp32_model = torch_tensorrt.compile(model, **compile_spec)\n",
    "test_loss, test_acc = evaluate(trt_fp32_model, val_dataloader, criterion)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =    2.33 ms\tmax =    5.37 ms\tavg =    2.61 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(trt_fp32_model, nruns=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上可以看出直接输入model，让`torch_tensorrt.compile`来自动处理是最方便的，性能方面也无需顾虑。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRT FP16 model on CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_model = torch.jit.script(model).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Check verbose logs for the list of affected weights.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 76 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 17 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_s1 script accuracy: 83.52%\n"
     ]
    }
   ],
   "source": [
    "# benchmark the performance of the baseline TRT model (TRT FP16 Model)\n",
    "compile_spec = {\n",
    "    \"inputs\": [torch_tensorrt.Input([1, 3, 224, 224])],\n",
    "    \"enabled_precisions\": {torch.half},  # Run with fp16\n",
    "    }\n",
    "trt_gpu_fp16_model = torch_tensorrt.compile(model, **compile_spec)\n",
    "test_loss, test_acc = evaluate(trt_gpu_fp16_model, val_dataloader, criterion)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =    1.86 ms\tmax =    5.96 ms\tavg =    2.22 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(trt_gpu_fp16_model, nruns=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Mean converter disregards dtype\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - TensorRT encountered issues when converting weights between types and that could affect accuracy.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - If this is not the desired behavior, please modify the weights or retrain with regularization to adjust the magnitude of the weights.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - Check verbose logs for the list of affected weights.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 76 weights are affected by this issue: Detected subnormal FP16 values.\n",
      "WARNING: [Torch-TensorRT TorchScript Conversion Context] - - 17 weights are affected by this issue: Detected values less than smallest positive FP16 subnormal value and converted them to the FP16 minimum subnormalized value.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_s1 script accuracy: 83.57%\n"
     ]
    }
   ],
   "source": [
    "spec = {\n",
    "    \"forward\": torch_tensorrt.ts.TensorRTCompileSpec(\n",
    "        **{\n",
    "            \"inputs\": [torch_tensorrt.Input([1, 3, 224, 224])],\n",
    "            \"enabled_precisions\": {torch.half},\n",
    "            \"refit\": False,\n",
    "            \"debug\": False,\n",
    "            \"device\": {\n",
    "                \"device_type\": torch_tensorrt.DeviceType.GPU,\n",
    "                \"gpu_id\": 0,\n",
    "                \"dla_core\": 0,\n",
    "                \"allow_gpu_fallback\": True,\n",
    "            },\n",
    "            \"capability\": torch_tensorrt.EngineCapability.default,\n",
    "            \"num_avg_timing_iters\": 1,\n",
    "        }\n",
    "    )\n",
    "}\n",
    "gpu_fp16_model = torch._C._jit_to_backend(\"tensorrt\", script_model, spec)\n",
    "test_loss, test_acc = evaluate(gpu_fp16_model, val_dataloader, criterion)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =    1.88 ms\tmax =    4.32 ms\tavg =    2.33 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(gpu_fp16_model, nruns=2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DLA part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "efficientformerv2_s1 script accuracy: 82.83%\n"
     ]
    }
   ],
   "source": [
    "# benchmark the performance of the baseline TRT model (TRT FP16 Model)\n",
    "if False:\n",
    "    # very curious why the following code stuck!!!\n",
    "    compile_spec = {\n",
    "        \"inputs\": [torch_tensorrt.Input([1, 3, 224, 224])],\n",
    "        \"enabled_precisions\": {torch.half},  # Run with FP16\n",
    "        \"device\": torch_tensorrt.Device(\"dla:0\", allow_gpu_fallback=True)  # Run with DLA\n",
    "        }\n",
    "    trt_dla_fp16_model = torch_tensorrt.compile(model, **compile_spec)\n",
    "else:\n",
    "    # python ~/work/transfer-learning/EfficientFormerV2/predict.py --benchmark --weights efficientformerv2_s1_base_ckpt.pth --factor s1 --num_classes 10 --device cuda --mode trt --test_iter 1000\n",
    "    trt_dla_fp16_model = torch.jit.load(\"trt_dla_fp16_model.ts\")\n",
    "test_loss, test_acc = evaluate(trt_dla_fp16_model, val_dataloader, criterion, False)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =   25.87 ms\tmax =   64.58 ms\tavg =   26.93 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(trt_dla_fp16_model, nruns=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = {\n",
    "    \"forward\": torch_tensorrt.ts.TensorRTCompileSpec(\n",
    "        **{\n",
    "            \"inputs\": [torch_tensorrt.Input([1, 3, 224, 224])],\n",
    "            \"enabled_precisions\": {torch.half},\n",
    "            \"refit\": False,\n",
    "            \"debug\": False,\n",
    "            \"device\": {\n",
    "                \"device_type\": torch_tensorrt.DeviceType.DLA,\n",
    "                \"gpu_id\": 0,\n",
    "                \"dla_core\": 0,\n",
    "                \"allow_gpu_fallback\": True,\n",
    "            },\n",
    "            \"capability\": torch_tensorrt.EngineCapability.default,\n",
    "            \"num_avg_timing_iters\": 1,\n",
    "        }\n",
    "    )\n",
    "}\n",
    "dla_fp16_model = torch._C._jit_to_backend(\"tensorrt\", script_model, spec)\n",
    "test_loss, test_acc = evaluate(dla_fp16_model, val_dataloader, criterion, False)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))\n",
    "# efficientformerv2_s1 script accuracy: 82.83%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min =   26.01 ms\tmax =   52.46 ms\tavg =   26.70 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(dla_fp16_model, nruns=400)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFT int 8 model on CUDA | Post Training Quantization (PTQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calib_dataloader = data.DataLoader(calib_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "calibrator = torch_tensorrt.ptq.DataLoaderCalibrator(\n",
    "    calib_dataloader,\n",
    "    use_cache=False,\n",
    "    algo_type=torch_tensorrt.ptq.CalibrationAlgo.ENTROPY_CALIBRATION_2,\n",
    "    device=torch.device('cuda'))\n",
    "\n",
    "compile_spec = {\n",
    "         \"inputs\": [torch_tensorrt.Input([64, 3, 224, 224])],\n",
    "         \"enabled_precisions\": torch.int8,\n",
    "         \"calibrator\": calibrator,\n",
    "     }\n",
    "ptq_int8_model = torch_tensorrt.compile(model, **compile_spec)\n",
    "\n",
    "test_loss, test_acc = evaluate(ptq_int8_model, val_dataloader, criterion, 0)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(ptq_int8_model, nruns=2000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TFT int 8 model on CUDA | Quantization Aware Training (QAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_modules.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function allows you to set the all the parameters to not have gradients, \n",
    "#allowing you to freeze the model and not undergo training during the train step. \n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the regular conv, FC layers will be converted to their quantized counterparts due to quant_modules.initialize()\n",
    "# feature_extract = False\n",
    "create_model = getattr(efficientformerv2, model_name)\n",
    "q_model = create_model(num_classes=10).cuda()\n",
    "# set_parameter_requires_grad(q_model, feature_extract)\n",
    "\n",
    "# mobilenetv2_base_ckpt is the checkpoint generated from Step 2 : Training a baseline Mobilenetv2 model.\n",
    "ckpt = torch.load(model_name+\"_base_ckpt.pth\")\n",
    "modified_state_dict={}\n",
    "for key, val in ckpt[\"model\"].items():\n",
    "    # Remove 'module.' from the key names\n",
    "    if key.startswith('module'):\n",
    "        modified_state_dict[key[7:]] = val\n",
    "    else:\n",
    "        modified_state_dict[key] = val\n",
    "\n",
    "# Load the pre-trained checkpoint\n",
    "q_model.load_state_dict(modified_state_dict)\n",
    "lr = 0.0001\n",
    "optimizer = optim.SGD(q_model.parameters(), lr=lr)\n",
    "optimizer.load_state_dict(ckpt[\"opt_state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_amax(model, **kwargs):\n",
    "    # Load calib result\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                if isinstance(module._calibrator, calib.MaxCalibrator):\n",
    "                    module.load_calib_amax()\n",
    "                else:\n",
    "                    module.load_calib_amax(**kwargs)\n",
    "    model.cuda()\n",
    "\n",
    "def collect_stats(model, data_loader, num_batches):\n",
    "    \"\"\"Feed data to the network and collect statistics\"\"\"\n",
    "    # Enable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.disable_quant()\n",
    "                module.enable_calib()\n",
    "            else:\n",
    "                module.disable()\n",
    "\n",
    "    # Feed data to the network for collecting stats\n",
    "    for i, (image, _) in tqdm(enumerate(data_loader), total=num_batches):\n",
    "        model(image.cuda())\n",
    "        if i >= num_batches:\n",
    "            break\n",
    "\n",
    "    # Disable calibrators\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, quant_nn.TensorQuantizer):\n",
    "            if module._calibrator is not None:\n",
    "                module.enable_quant()\n",
    "                module.disable_calib()\n",
    "            else:\n",
    "                module.enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calibrate the model using percentile calibration technique.\n",
    "with torch.no_grad():\n",
    "    collect_stats(q_model, train_dataloader, num_batches=32)\n",
    "    compute_amax(q_model, method=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finetune the QAT model for 2 epochs\n",
    "train_dataloader = data.DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=64, shuffle=False, drop_last=True)\n",
    "num_epochs=10\n",
    "lr = 0.001\n",
    "best_acc = 0.\n",
    "for epoch in range(num_epochs):\n",
    "    print('Epoch: [%5d / %5d] LR: %f' % (epoch + 1, num_epochs, lr))\n",
    "\n",
    "    train(q_model, train_dataloader, criterion, optimizer, epoch)\n",
    "    test_loss, test_acc = evaluate(q_model, val_dataloader, criterion, epoch)\n",
    "\n",
    "    print(\"Test Loss: {:.5f} Test Acc: {:.2f}%\".format(test_loss, 100 * test_acc))\n",
    "    if best_acc < test_acc:\n",
    "        best_acc = test_acc\n",
    "        save_checkpoint({'epoch': epoch + 1,\n",
    "                 'model': q_model.state_dict(),\n",
    "                 'acc': best_acc,\n",
    "                 'opt_state_dict': optimizer.state_dict()\n",
    "                },\n",
    "                ckpt_path=model_name+\"_qat_ckpt.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_nn.TensorQuantizer.use_fb_fake_quant = True\n",
    "with torch.no_grad():\n",
    "    jit_model = torch.jit.script(q_model, torch.randn((1,3,224,224)).cuda())\n",
    "    torch.jit.save(jit_model, model_name+\"_qat.jit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qat_model = torch.jit.load(model_name+\"_qat.jit.pt\").eval()\n",
    "compile_spec = {\"inputs\": [torch_tensorrt.Input([1, 3, 224, 224])],\n",
    "                \"enabled_precisions\": torch.int8,\n",
    "                # \"truncate_long_and_double\": True,\n",
    "               }\n",
    "qat_int8_model = torch_tensorrt.compile(q_model.eval(), **compile_spec)\n",
    "val_dataloader = data.DataLoader(val_dataset, batch_size=1, shuffle=False, drop_last=True)\n",
    "test_loss, test_acc = evaluate(qat_int8_model, val_dataloader, criterion, 0)\n",
    "print(model_name+\" script accuracy: {:.2f}%\".format(100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark(qat_int8_model, nruns=2000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
